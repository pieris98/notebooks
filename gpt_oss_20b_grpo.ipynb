{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pieris98/notebooks/blob/main/gpt_oss_20b_grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRwpsC9d5II7"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJtskWGr5IJE"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-qadg9k5IJF"
      },
      "source": [
        "\n",
        "[Vision RL](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) is now supported! Train Qwen2.5-VL, Gemma 3 etc. with GSPO or GRPO.\n",
        "\n",
        "Introducing Unsloth [Standby for RL](https://docs.unsloth.ai/basics/memory-efficient-rl): GRPO is now faster, uses 30% less memory with 2x longer context.\n",
        "\n",
        "Gpt-oss fine-tuning now supports 8√ó longer context with 0 accuracy loss. [Read more](https://docs.unsloth.ai/basics/long-context-gpt-oss-training)\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPa6BvC5IJH"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCa6z6np5IJL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!\n",
        "!pip install --upgrade -qqq uv\n",
        "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "except: get_numpy = \"numpy\"\n",
        "!uv pip install -qqq \\\n",
        "    \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers>=4.55.3\" \\\n",
        "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkH_y8UC9lvv"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPgFeIkZn9q"
      },
      "source": [
        "# Goal: Make faster kernels with Reinforcement Learning\n",
        "\n",
        "Our goal is to make a faster matrix multiplication kernel by doing RL on GTP-OSS 20B with Unsloth.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Matrix_multiplication_qtl1.svg/500px-Matrix_multiplication_qtl1.svg.png\" height=200 />\n",
        "\n",
        "You will learn how to:\n",
        "1. Counteract **reward hacking** like cheating, caching, laziness.\n",
        "2. Timing and correctness of kernels and time limits.\n",
        "3. Making good **reward functions**\n",
        "4. How to seriously do RL to make optimized CUDA kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "8816145d9ebc41de85c4c1ee9de108e6",
            "f729f94a15b04990973031c83a627913",
            "cbc537fa67134dd29f88c677fc7f68ed",
            "e2b4c5f860a04ad5810462d3c09b94d6",
            "0771c590a7984c78bd2286bd443df77f",
            "60d3ef36d2ec44aebde286d5d3ed663a",
            "be4a8af1b18d40599be7ef126013a003",
            "519595c593e4469cba2a631bc55169c5",
            "1adf0184f0224fe396ecc7cc8bcbee1f",
            "e2b23938977b40fc8c93885a3141edc0",
            "b646884b63dc4a14bcd3b04e31c29505"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "7ff4c22c-7952-41b5-883f-3a729acf9837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.9: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gpt_oss won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8816145d9ebc41de85c4c1ee9de108e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Offloading embeddings to RAM to save 1.08 GB.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 640 # Can increase for longer RL output\n",
        "lora_rank = 4 # Larger rank = smarter, but slower\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gpt-oss-20b\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    offload_embedding = True, # Reduces VRAM by 1GB\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now add some small amount of LoRA weights to GPT-OSS so we only need to train those, instead of training on the full model."
      ],
      "metadata": {
        "id": "TfeUs-lQJDSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rGa-o3HJCo1",
        "outputId": "1bfc3950-4b87-4bd1-f5b8-bf9af5b91dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimized matrix multiplication\n",
        "\n",
        "Numpy has optimized matrix multiplication kernels for CPUs via BLAS optimized operations. For GPUs, one can use CUDA accelerated cuBLAS kernels which PyTorch calls under the hood.\n",
        "\n",
        "To generate some random matrices to do matrix multiplication, we can do the below:"
      ],
      "metadata": {
        "id": "N0QnO9_YJBOI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9CI4jtgL5mw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def generate_random_matrices(seed = 3407, n = 256):\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    n, k, m = random_state.randint(1, n+1, size = 3)\n",
        "    A = np.random.uniform(-10, 10, size = (n, k))\n",
        "    B = np.random.uniform(-10, 10, size = (k, m))\n",
        "    return A, A.tolist(), B, B.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall generate a small matrix, and see the matrix multiplied output"
      ],
      "metadata": {
        "id": "4BcaLniVKLpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A, A_list, B, B_list = generate_random_matrices(seed = 42, n = 5)\n",
        "print(A)\n",
        "print(B)\n",
        "print(np.matmul(A, B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M8kGaFRJ2ic",
        "outputId": "e1b35afc-b02d-4d0f-d450-fdbe2b9a3aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.8313286   4.54613909 -7.95265309  6.53459836  2.87235103]\n",
            " [ 7.0739631   3.76278879  9.31565599 -8.52884711  9.96832952]\n",
            " [ 8.41214082  6.51136046 -3.79347975 -2.46773693 -2.32292989]\n",
            " [ 3.91302932  4.98335304 -5.33855089  5.71057634 -2.79871647]]\n",
            "[[ 0.39218774 -9.6181377  -3.49736707]\n",
            " [-0.33354865 -1.05626139  3.87231208]\n",
            " [ 0.49494174  5.91863954 -6.83183693]\n",
            " [ 5.1465162  -7.51648113  1.00445384]\n",
            " [ 9.63213377 -4.92327556  3.323014  ]]\n",
            "[[  54.73441488  -87.89725072   97.94605887]\n",
            " [  58.25238906   -1.8467447   -49.25453031]\n",
            " [ -35.82528794  -80.25394462   11.51225408]\n",
            " [  -0.33785799 -103.64132345   38.51974367]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call a LLM to generate a simple matrix multiply kernel in Python only, and we can calculate the differences between the actual result and the kernel's result"
      ],
      "metadata": {
        "id": "envzrXmjKRff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_difference(pred, real):\n",
        "    if pred is None: return 3, 3\n",
        "    assert real is not None\n",
        "    import numpy as np\n",
        "    difference = pred - real\n",
        "    amax_error = float(np.amax(difference))\n",
        "    mse_error  = float(np.mean(np.square(difference)))\n",
        "    return amax_error, mse_error"
      ],
      "metadata": {
        "id": "b-gSgthFI_wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel generated by GPT-5\n",
        "def matmul(A, B):\n",
        "    z, s = zip, sum\n",
        "    Bt = list(z(*B))\n",
        "    return [[s(a*b for a, b in z(row, col)) for col in Bt] for row in A]"
      ],
      "metadata": {
        "id": "q9gmkbTnKbcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the error below is very small, so that's good!"
      ],
      "metadata": {
        "id": "J-WfRwQeKtEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = matmul(A_list, B_list)\n",
        "calculate_difference(prediction, np.matmul(A, B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QvIidsPKg2C",
        "outputId": "0bc6e685-f913-4814-bd18-9ddc831e8f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7.105427357601002e-15, 4.6783406255758477e-29)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR6czU96cpxf"
      },
      "source": [
        "# Countering Reward Hacking\n",
        "\n",
        "The ultimate goal of RL is to maximize some reward (say speed, revenue, some metric).\n",
        "\n",
        "But RL can **cheat** When the RL algorithm learns a trick or exploits something to increase the reward, without actually doing the task at end, this is called \"Reward Hacking\".\n",
        "\n",
        "Some good examples are in https://en.wikipedia.org/wiki/Reward_hacking\n",
        "\n",
        "For matrix multiplication kernels, we might see the following issues:\n",
        "\n",
        "* Laziness: RL learns to use Numpy, Torch, other libraries, which calls optimized CUDA kernels.\n",
        "* Caching: RL learns to cache the result of the output\n",
        "* Cheating: RL learns to find the actual output by inspecting Python global variables\n",
        "* RL learns to edit the timing function to make it output 0 time as passed.\n",
        "\n",
        "And possibly more. We shall try to address each!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countering Reward Hacking 1: Stop laziness\n",
        "We can stop the RL algorithm from calling optimized code by inspecting if the generated code imports other non standard Python libraries. We used GPT-5 to help generate this check `check_only_stdlib_imports`:"
      ],
      "metadata": {
        "id": "tRhLV_bZMYxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Collapsible code)\n",
        "import ast\n",
        "import sys\n",
        "import sysconfig\n",
        "from pathlib import Path\n",
        "\n",
        "def _stdlib_names():\n",
        "    \"\"\"\n",
        "    Build a set of canonical stdlib top-level module/package names.\n",
        "    Uses sys.stdlib_module_names when available (3.10+), with a\n",
        "    filesystem fallback for older versions/edge cases.\n",
        "    \"\"\"\n",
        "    names = {m.lower() for m in getattr(sys, \"stdlib_module_names\", set())}\n",
        "    names |= {m.lower() for m in sys.builtin_module_names}\n",
        "    names.add(\"__future__\")  # special-case\n",
        "\n",
        "    # Fallback/augmentation: scan the stdlib directory\n",
        "    try:\n",
        "        stdlib_dir = Path(sysconfig.get_path(\"stdlib\"))\n",
        "        if stdlib_dir.exists():\n",
        "            for p in stdlib_dir.iterdir():\n",
        "                if p.name == \"site-packages\":\n",
        "                    continue\n",
        "                if p.suffix == \".py\":\n",
        "                    names.add(p.stem.lower())\n",
        "                elif p.is_dir() and (p / \"__init__.py\").exists():\n",
        "                    names.add(p.name.lower())\n",
        "    except Exception:\n",
        "        # conservative fallback; the names set above will still work well\n",
        "        pass\n",
        "\n",
        "    return names\n",
        "\n",
        "_STDLIB_SET = _stdlib_names()\n",
        "\n",
        "def check_only_stdlib_imports(code: str):\n",
        "    \"\"\"\n",
        "    Return (ok: bool, details: dict)\n",
        "\n",
        "    ok == True  -> all absolute imports are from the stdlib.\n",
        "    ok == False -> details['non_stdlib'] lists offending top-level modules.\n",
        "\n",
        "    details includes:\n",
        "      - stdlib: sorted list of stdlib imports found\n",
        "      - non_stdlib: sorted list of non-stdlib imports found\n",
        "      - relative_imports: count of relative imports (always allowed here)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "    except SyntaxError as e:\n",
        "        return False, {\n",
        "            \"error\": f\"SyntaxError: {e}\",\n",
        "            \"stdlib\": [],\n",
        "            \"non_stdlib\": [],\n",
        "            \"relative_imports\": 0,\n",
        "        }\n",
        "\n",
        "    abs_imports = set()\n",
        "    relative_count = 0\n",
        "\n",
        "    class Visitor(ast.NodeVisitor):\n",
        "        def visit_Import(self, node: ast.Import):\n",
        "            for alias in node.names:\n",
        "                abs_imports.add(alias.name.split(\".\")[0])\n",
        "        def visit_ImportFrom(self, node: ast.ImportFrom):\n",
        "            nonlocal relative_count\n",
        "            if (node.level or 0) > 0:\n",
        "                # relative import\n",
        "                relative_count += 1\n",
        "            else:\n",
        "                if node.module:\n",
        "                    abs_imports.add(node.module.split(\".\")[0])\n",
        "\n",
        "    Visitor().visit(tree)\n",
        "\n",
        "    stdlib_found = sorted(m for m in abs_imports if m.lower() in _STDLIB_SET)\n",
        "    non_stdlib = sorted(m for m in abs_imports if m.lower() not in _STDLIB_SET)\n",
        "\n",
        "    return len(non_stdlib) == 0, {\n",
        "        \"stdlib\": stdlib_found,\n",
        "        \"non_stdlib\": non_stdlib,\n",
        "        \"relative_imports\": relative_count,\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MXPZ1MsUMqNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let's call `check_only_stdlib_imports` on a random piece of matrix multiplication code generated by GPT-5:"
      ],
      "metadata": {
        "id": "ngUAw1lMM9JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"\"\"\n",
        "def matmul(A, B):\n",
        "    import numpy as np\n",
        "    from torch import matmul\n",
        "    z, s = zip, sum\n",
        "    Bt = list(z(*B))\n",
        "    return [[s(a*b for a, b in z(row, col)) for col in Bt] for row in A]\n",
        "\"\"\"\n",
        "ok, info = check_only_stdlib_imports(sample)\n",
        "print(\"Only stdlib imports?\", ok)\n",
        "print(info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz80kvg6M4BG",
        "outputId": "7ce850f0-88a5-473c-fbba-d5b9e7d53542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only stdlib imports? False\n",
            "{'stdlib': [], 'non_stdlib': ['numpy', 'torch'], 'relative_imports': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countering Reward Hacking 2: Stop cheating\n",
        "We can stop the RL algorithm from using global or cached variables by restricting it's `locals` and `globals`.\n",
        "\n",
        "We are also going to use `exec` to create the function, so we have to save the output to an empty dict.\n",
        "\n",
        "We also disallow global variable access."
      ],
      "metadata": {
        "id": "J6lgkGkEN7B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_function = {}\n",
        "exec(sample, {}, output_function)\n",
        "output_function[\"matmul\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrIeYu-lOLSv",
        "outputId": "61c5d8ce-d9c3-4c5e-953f-8c54ef3d8d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matmul(A, B)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also disallow global variable access via `types.FunctionType(f.__code__, {})`"
      ],
      "metadata": {
        "id": "SDSrjOTLVyQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "output_function[\"matmul\"] = types.FunctionType(output_function[\"matmul\"].__code__, {})\n",
        "\n",
        "def import_numpy():\n",
        "    np.matmul\n",
        "    print(\"Success\")\n",
        "\n",
        "import_numpy()\n",
        "import_numpy = types.FunctionType(import_numpy.__code__, {})\n",
        "try:\n",
        "    import_numpy()\n",
        "except Exception as e:\n",
        "    print(str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcmYAmohVqw2",
        "outputId": "58c6f176-6034-48df-9354-f5a3b5b3a024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n",
            "name 'np' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_locked_down_function(function):\n",
        "    output_function = {}\n",
        "    exec(function, {}, output_function)\n",
        "    new_matmul = output_function[\"matmul\"]\n",
        "    new_matmul = types.FunctionType(new_matmul.__code__, {})\n",
        "    return new_matmul"
      ],
      "metadata": {
        "id": "5tJKwLUgZsRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countering Reward Hacking 3: Stop caching\n",
        "We can stop the RL algorithm from using cached data by wiping the cache with a large fake matrix. We also have to benchmark carefully with multiple loops and turns.\n",
        "\n",
        "We also add a **timer** to not make the algorithm go in an endless loop."
      ],
      "metadata": {
        "id": "Sl-IxTZ6Nvm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, time, statistics\n",
        "import signal\n",
        "from contextlib import contextmanager\n",
        "class TimeoutError(Exception): pass\n",
        "\n",
        "@contextmanager\n",
        "def time_limit(seconds):\n",
        "    def _handler(signum, frame):\n",
        "        raise TimeoutError(f\"Timed out after {seconds}s\")\n",
        "    old = signal.signal(signal.SIGALRM, _handler)\n",
        "    signal.setitimer(signal.ITIMER_REAL, seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.setitimer(signal.ITIMER_REAL, 0.0)\n",
        "        signal.signal(signal.SIGALRM, old)\n",
        "\n",
        "class Benchmarker:\n",
        "    def __init__(self, trials = 3, loops = 1, timeout = 30):\n",
        "        self.buffer = np.zeros(2 * 1024 * 1024 * 1024, dtype = np.uint8)\n",
        "        self.trials = trials\n",
        "        self.loops = loops\n",
        "        assert timeout > 0 # Cannot be 0 since it won't work!\n",
        "        self.timeout = timeout\n",
        "    def thrash(self):\n",
        "        # Edit the buffer to wipe cache lines\n",
        "        self.buffer ^= 1\n",
        "        return int(self.buffer[::4096].sum())\n",
        "\n",
        "    def benchmark(self, function, arguments):\n",
        "        assert len(arguments) == self.loops\n",
        "        samples = []\n",
        "        exceptions = []\n",
        "        timed_out = 0\n",
        "        for _ in range(self.trials):\n",
        "            gc.collect(); gc.disable(); self.thrash()\n",
        "            t_start = time.perf_counter_ns()\n",
        "            for i in range(self.loops):\n",
        "                try:\n",
        "                    with time_limit(self.timeout):\n",
        "                        function(*arguments[i])\n",
        "                except TimeoutError as e:\n",
        "                    timed_out += 1\n",
        "                except Exception as e:\n",
        "                    exceptions.append(str(e))\n",
        "            t_end = time.perf_counter_ns()\n",
        "            gc.enable()\n",
        "            samples.append((t_end - t_start) // max(1, self.loops))\n",
        "        return {\n",
        "            \"median_ns\": int(statistics.median(samples)),\n",
        "            \"mean_ns\": int(statistics.fmean(samples)),\n",
        "            \"stdev_ns\": int(statistics.pstdev(samples) if len(samples) > 1 else 0),\n",
        "            \"exceptions\" : exceptions,\n",
        "            \"timeouts\" : timed_out,\n",
        "        }"
      ],
      "metadata": {
        "id": "jZwGdNyMNlEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example we use our matmul kernel we had, and benchmark it with a 10 second delay:"
      ],
      "metadata": {
        "id": "PV5M0DCyOvon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A, A_list, B, B_list = generate_random_matrices(seed = 0, n = 256)\n",
        "Benchmarker(trials = 1, timeout = 10).benchmark(output_function[\"matmul\"], [(A_list, B_list)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8df8tZcEOuYJ",
        "outputId": "573b1d99-76ed-4e10-afc2-745f67fb7bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'median_ns': 141363816,\n",
              " 'mean_ns': 141363816,\n",
              " 'stdev_ns': 0,\n",
              " 'exceptions': [],\n",
              " 'timeouts': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data & RL task setup\n",
        "\n",
        "We now have to create a prompt to the model for which it will do some task. For our matrix multiply example, we use the below:"
      ],
      "metadata": {
        "id": "8CzwCyXIPK04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-2RRE4HMrQO",
        "outputId": "3b64aa5d-a3c8-4085-8550-5eef78c476b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create a new fast matrix multiplication function using only native Python code.\n",
            "You are given a list of list of numbers.\n",
            "Output your function in backticks using the format below:\n",
            "```python\n",
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Create a new fast matrix multiplication function using only native Python code.\n",
        "You are given a list of list of numbers.\n",
        "Output your function in backticks using the format below:\n",
        "```python\n",
        "def matmul(A, B):\n",
        "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n",
        "```\n",
        "\"\"\".strip()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIdudFUodN4i"
      },
      "source": [
        "First, let's prompt GPT-OSS without RL and see how it goes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HJxrS76h3Ds",
        "outputId": "0caa5201-0f0e-4bfc-d6f9-621bac458220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-09-26\n",
            "\n",
            "Reasoning: low\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Create a new fast matrix multiplication function using only native Python code.\n",
            "You are given a list of list of numbers.\n",
            "Output your function in backticks using the format below:\n",
            "```python\n",
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n",
            "```<|end|><|start|>assistant<|channel|>analysis<|message|>User wants the code.<|end|><|start|>assistant<|channel|>final<|message|>```python\n",
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n",
            "```<|return|>\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": prompt}],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    reasoning_effort = \"low\",\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 1.0,\n",
        "    max_new_tokens = 512,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iknaWZNudTNq"
      },
      "source": [
        "# Reward functions\n",
        "\n",
        "We now design the `extract_function` function which simply extracts the function wrapped in 3 backticks.\n",
        "\n",
        "And 4 reward functions:\n",
        "\n",
        "1. `function_works` which rewards the model if the strategy is a valid Python function.\n",
        "2. `no_cheating` which checks if the function imported other modules, and if it did, we penalize it.\n",
        "3. `correctness_check` which checks if the kernel was correct or wrong - it shouldn't generate gibberish!\n",
        "4. `speed_check` checks the performance relative to Numpy matmul directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JJGXKdJ-Zl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7cfdb80-1408-45c0-f375-46fd0ce479b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n"
          ]
        }
      ],
      "source": [
        "def extract_function(text):\n",
        "    if text.count(\"```\") >= 2:\n",
        "        first = text.find(\"```\") + 3\n",
        "        second = text.find(\"```\", first)\n",
        "        fx = text[first : second].strip()\n",
        "        fx = fx.removeprefix(\"python\\n\")\n",
        "        fx = fx[fx.find(\"def\"):]\n",
        "        if fx.startswith(\"def matmul(A, B):\"): return fx\n",
        "    return None\n",
        "print(extract_function(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is our `function_works` reward function which uses Python's `exec` but guarded by not allowing leakage of local and global variables. We can also use `check_only_stdlib_imports` first to check if there are errors before even executing the function:"
      ],
      "metadata": {
        "id": "KLXEcf_HSJlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ok, info = check_only_stdlib_imports(\"def a\")\n",
        "ok, info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3-B0IIsS56S",
        "outputId": "d1dfb200-2ff0-4501-f04b-6e218804f582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " {'error': \"SyntaxError: expected '(' (<unknown>, line 1)\",\n",
              "  'stdlib': [],\n",
              "  'non_stdlib': [],\n",
              "  'relative_imports': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def function_works(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        print(function)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        if function is None or \"error\" in info:\n",
        "            score = -2.0\n",
        "        else:\n",
        "            try:\n",
        "                new_matmul = create_locked_down_function(function)\n",
        "                score = 1.0\n",
        "            except:\n",
        "                score = -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "`no_cheating` checks if the function cheated since it might have imported Numpy or Torch optimized code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def no_cheating(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        else:\n",
        "            ok = False\n",
        "        scores.append(1.0 if ok else -20.0) # Penalize heavily!\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next `correctness_check` checks if the kernel was correct. We want to penalize if the absolute error is larger than 1, and if the mean squared error is somewhat bigger then machine epsilon.\n",
        "\n",
        "We have to execute the code now!"
      ],
      "metadata": {
        "id": "slnqWG3FTror"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.finfo(np.float64).eps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFBp-MkyYeoE",
        "outputId": "d7c26969-60d8-4f64-ab78-1fb2559c9b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(2.220446049250313e-16)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correctness_check(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        if function is None or \"error\" in info:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            new_matmul = create_locked_down_function(function)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Generate some random matrices of size less than 128\n",
        "        A, A_list, B, B_list = generate_random_matrices(seed = np.random.randint(10000), n = 128)\n",
        "        try:\n",
        "            pred = new_matmul(A_list, B_list)\n",
        "        except:\n",
        "            # Failed!\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        true = np.matmul(A, B)\n",
        "        amax_error, mse_error = calculate_difference(pred, true)\n",
        "\n",
        "        # Check correctness and score!\n",
        "        machine_epsilon = 100*np.finfo(np.float64).eps\n",
        "        if   amax_error >= 3:   score = -3.0\n",
        "        elif amax_error >= 2:   score = -2.5\n",
        "        elif amax_error >= 1:   score = -2.0\n",
        "        elif amax_error >= 0.5: score = -1.0\n",
        "        elif amax_error >= 100*machine_epsilon: score = 0.0\n",
        "        elif amax_error >= machine_epsilon: score = 1.0\n",
        "        else: score = 3.0\n",
        "\n",
        "        if   mse_error >= 3:   score += -3.0\n",
        "        elif mse_error >= 2:   score += -2.5\n",
        "        elif mse_error >= 1:   score += -2.0\n",
        "        elif mse_error >= 0.5: score += -1.0\n",
        "        elif mse_error >= 100*machine_epsilon: score += 0.0\n",
        "        elif mse_error >= machine_epsilon: score += 1.0\n",
        "        else: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "sNi129lYTpZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally our benchmarking function for `speed_check`! We shall limit the timer to 10 seconds and do 3 trials."
      ],
      "metadata": {
        "id": "CpTrofI9ZIn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A, A_list, B, B_list = generate_random_matrices(seed = 0, n = 256)\n",
        "benchmarker = Benchmarker(trials = 3, timeout = 10)\n",
        "numpy_results = benchmarker.benchmark(np.matmul, [(A, B)])\n",
        "numpy_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5xkIAzuZMnO",
        "outputId": "1005260d-d320-47dd-8044-2f73eb86b295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'median_ns': 1190190,\n",
              " 'mean_ns': 2582107,\n",
              " 'stdev_ns': 2645072,\n",
              " 'exceptions': [],\n",
              " 'timeouts': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_matmul = create_locked_down_function(extract_function(prompt))\n",
        "new_results = benchmarker.benchmark(new_matmul, [(A_list, B_list)])\n",
        "new_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNDc6skFZZW6",
        "outputId": "30e8358e-1d86-47bb-9e34-81a3cf9c3914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'median_ns': 74952790,\n",
              " 'mean_ns': 74854853,\n",
              " 'stdev_ns': 596015,\n",
              " 'exceptions': [],\n",
              " 'timeouts': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take the difference and do a negative sign for slower ones. If the ratio is less than 1 (ie faster, we shall invert it!)"
      ],
      "metadata": {
        "id": "noUAaX24aqNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
        "positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
        "reward = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IT9nXcjaI-X",
        "outputId": "6c88e7ee-922d-4a39-b38d-f8fcedabb420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.6297548290609063"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_results[\"median_ns\"] = 3\n",
        "numpy_results[\"median_ns\"] = 1000\n",
        "negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
        "positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
        "reward = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntYNFV0ra-MX",
        "outputId": "f090bd8f-c004-4a71-e6ce-87bc14784c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def speed_check(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        if function is None or \"error\" in info:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            new_matmul = create_locked_down_function(function)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Generate some random matrices of size less than 256\n",
        "        A, A_list, B, B_list = generate_random_matrices(seed = np.random.randint(10000), n = 256)\n",
        "        numpy_results = benchmarker.benchmark(np.matmul,  [(A, B)])\n",
        "        new_results   = benchmarker.benchmark(new_matmul, [(A_list, B_list)])\n",
        "\n",
        "        # Get score and clip to -10, 10\n",
        "        negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
        "        positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
        "        score = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
        "        if score >= 10:  score = 10\n",
        "        if score <= -10: score = -10\n",
        "        scores.append(score)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "HHmXmAxtbVpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCpSxtvSeAG_"
      },
      "source": [
        "We create the dataset which includes a replica of our prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldf6SjLHVPRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62982a19-9ef0-444b-9521-8138ee7ffa16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': [{'content': 'Create a new fast matrix multiplication function using only native Python code.\\nYou are given a list of list of numbers.\\nOutput your function in backticks using the format below:\\n```python\\ndef matmul(A, B):\\n    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\\n```',\n",
              "   'role': 'user'}],\n",
              " 'answer': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list([{\"prompt\" : [{\"role\": \"user\", \"content\": prompt.strip()}], \"answer\" : 0}]*1000)\n",
        "maximum_length = len(tokenizer(prompt.strip())[\"input_ids\"])\n",
        "print(maximum_length)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations! We also support GSDP, GAPO, Dr GRPO and more! Go to our docs https://docs.unsloth.ai/ for more info!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a7f642-fcd8-462d-c318-1e8df1d24e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 2\n"
          ]
        }
      ],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-5,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 2, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 100,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac04e5e-eb4c-41c8-d27e-bbcd1eb5cee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        }
      ],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        function_works,\n",
        "        no_cheating,\n",
        "        correctness_check,\n",
        "        speed_check,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's train the model!\n",
        "\n",
        "**NOTE** A T4 free GPU might take 5 minutes for one generation sadly since it's an old GPU - A100 or H100 will be much faster!"
      ],
      "metadata": {
        "id": "fQhtuwP4cf34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "VGRxPdSCcfC3",
        "outputId": "7fa3c487-001f-4bf1-9fde-d0d316ff0e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 1,990,656 of 20,916,747,840 (0.01% trained)\n",
            "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 131072}. If this is not desired, please set these values explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/100 10:54 < 8:43:35, 0.00 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / function_works / mean</th>\n",
              "      <th>rewards / function_works / std</th>\n",
              "      <th>rewards / no_cheating / mean</th>\n",
              "      <th>rewards / no_cheating / std</th>\n",
              "      <th>rewards / correctness_check / mean</th>\n",
              "      <th>rewards / correctness_check / std</th>\n",
              "      <th>rewards / speed_check / mean</th>\n",
              "      <th>rewards / speed_check / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>391.500000</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.301827</td>\n",
              "      <td>1.987167</td>\n",
              "      <td>459.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.698173</td>\n",
              "      <td>1.987167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n",
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "def matmul(A, B):\n",
            "    # Ensure that the number of columns in A equals the number of rows in B\n",
            "    assert len(A[0]) == len(B), \"Incompatible dimensions\"\n",
            "\n",
            "    # Using zip(*B) to transpose B so we can iterate over columns\n",
            "    cols = list(zip(*B))\n",
            "    result = [[sum(a * b for a, b in zip(row, col)) for col in cols] for row in A]\n",
            "    return result\n",
            "def matmul(A, B):\n",
            "    return [[sum(a*b for a, b in zip(row, col)) for col in list(zip(*B))] for row in A]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "# Inference\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": prompt}],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    reasoning_effort = \"low\",\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 1.0,\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 or MXFP4 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `mxfp4` for MXFP4 (OpenAI's GPT-OSS native precision). We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge and push to hub in mxfp4 4bit format\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"mxfp4\")\n",
        "if False: model.push_to_hub_merged(\"repo_id/repo_name\", tokenizer, token = \"hf...\", save_method = \"mxfp4\")\n",
        "\n",
        "# Merge and push to hub in 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V15Yhj1V9lwG"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8816145d9ebc41de85c4c1ee9de108e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f729f94a15b04990973031c83a627913",
              "IPY_MODEL_cbc537fa67134dd29f88c677fc7f68ed",
              "IPY_MODEL_e2b4c5f860a04ad5810462d3c09b94d6"
            ],
            "layout": "IPY_MODEL_0771c590a7984c78bd2286bd443df77f"
          }
        },
        "f729f94a15b04990973031c83a627913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60d3ef36d2ec44aebde286d5d3ed663a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be4a8af1b18d40599be7ef126013a003",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "cbc537fa67134dd29f88c677fc7f68ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519595c593e4469cba2a631bc55169c5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1adf0184f0224fe396ecc7cc8bcbee1f",
            "value": 4
          }
        },
        "e2b4c5f860a04ad5810462d3c09b94d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b23938977b40fc8c93885a3141edc0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b646884b63dc4a14bcd3b04e31c29505",
            "value": "‚Äá4/4‚Äá[00:55&lt;00:00,‚Äá11.86s/it]"
          }
        },
        "0771c590a7984c78bd2286bd443df77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60d3ef36d2ec44aebde286d5d3ed663a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4a8af1b18d40599be7ef126013a003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "519595c593e4469cba2a631bc55169c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1adf0184f0224fe396ecc7cc8bcbee1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2b23938977b40fc8c93885a3141edc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b646884b63dc4a14bcd3b04e31c29505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}